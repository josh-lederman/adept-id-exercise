# adept-id-exercise memo
The core classification approach I used was to leverage information from one of two text fields in the dataset — the postings’ job titles or their body text. The script `predict_n_onets.py` provides the "final product" of the first deliverable requested, while `exercise_code.ipynb` gives some more context for how I arrived at and tested my approach.

In the early portion of my code, I performed some checks on the data for some previously experienced pitfalls of noisy text data, including missing values and/or poorly parsed titles and body text, but didn’t see many concerns there. Where there was some concern was in evaluating the target classes and their differences between the train and test sets. In both directions, there are occupations represented in one set but not the other, which would impose a hard cap on accuracy.

In the absence of many personal priors on specific LLMs, transformers, or encoders that would be best-suited to this task, I used the provided sentence-transformers implementation pre-trained on the all-MiniLM-L6-v2 model, as referenced in the exercise assignment, out of the box. Its embeddings of the titles and bodies were each used as the sole features for a suite of “One vs. Rest” multi-class algorithms — a decision tree, a Support Vector Classifier (SVC), and a multinomial logistic regression. The 3x2=6 trained classifiers were compared for accuracy on the test data; for this decision I decided not to delve into macro- or weighted-averaged versions of precision or recall, thinking that the misalignment of target sets mentioned above would be heavily penalized by those metrics.

Notably — and a bit surprisingly given the greater amount of text to work from in the bodies — the classifiers trained on title embeddings outperformed the body classifiers. I carried forward the title-based logistic regression for use in the predict function although it was slightly outperformed by the title-based SVC, because for this time-boxed exercise, it was easier to generate a top-N ranking with sklearn’s logistic regression implementation than its SVC implementation.

There are a few places I would go from here. First of all, more representative data across the target space is crucial for total accuracy; I would also take a closer look at `confusion_matrix.csv` included in the repo to find other trouble spots. Even if a sufficient number of job postings do not exist for certain ONETs, there may be other online sources that with terminology related to those ONETs. Data on some other posting elements, like source or employer, may also give a fuller picture.

Algorithmically, my original “either-or” approach of features based on titles vs. body text should obviously be refined. Likely the combined information, whether by looking at both embeddings as features in the same classifier, or by applying some sort of multi-layered ensemble of stacked encoders, could be leveraged for certain insights that would not come out of just looking at one or the other. Better tuning parameters in the existing candidate models, as well as exploring others, would be on the docket as well.

Finally, the question of how to define “accuracy” in the first place is very much not trivial, and likely depends on the use case. In particular, I find myself wondering about the fact that the `predict` function is built to suggest multiple labels, even though per the training and test data there is only one true right answer. Should evaluation be done in a way that rewards, or at least doesn’t as severely punish, a model for ranking a test case’s true ONET second or third? For picking an ONET that is extremely similar to the true ONET? Accounting for questions like these, likely with an eye toward the model’s specific applications, would be central to the process of evaluating and improving the model.
